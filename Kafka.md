### Внутренняя структура данных Kafka состоит из:
1. **Event (сообщение)**, который включает в себя: ключ (key), значение (value), timestamp и опциональный набор метаданных (headers);  
1. **Topics (топики)**, в которых организованы и хранятся сообщения. В свою очередь, каждый Topic состоит из одной или более партиций;
1. **Partitions (партиции)** - это распределенный отказоустойчивый лог (Log). Сообщения с одинаковыми ключами записываются в одну и ту же партицию;
1. У каждой партиции есть один брокер лидер - **Leader** (принимает сообщения от Producer и в общем случае отдает сообщения консьюмеру); 
1. **Фолловеры (Follower)** являются брокерами, которые хранят реплику всех данных партиции и осуществляют запросы лидеру.

#### Клиентские библиотеки:
Основные способы взаимодействия с кластером.  
Официальным языком и клиентом Kafka является Java. Для других языков также создано множество библиотек.  
Все брокеры кластера могут быть помещены за один IP/Load Balancer для Service Discovery. 
При этом нужно обеспечить прямой доступ клиентов к каждому из них.  
Подробное описание протокола Apache Kafka доступно в официальной документации: https://kafka.apache.org/protocol  

#### Producer:
Посылает сообщения брокеру батчами, улучшая пропускную способность и степень сжатия данных.  
linger.ms — аналог Nagle’s Algorithm из TCP.  
Внутренний буфер ограниченного размера — buffer.memory. При заполнении блокирует отправку на max.block.ms.  
delivery.timeout.ms — общий таймаут на доставку, равный 2-м минутам по умолчанию. В пределах этого лимита клиент будет пробовать доставить сообщение заново в случае ошибок.  
При помощи опции Retries можно контролировать количество попыток доставки. Важно помнить, что Retries могут сломать очередность сообщений если max.in.flight.requests.per.connection > 1.  
 
#### Как бороться с дубликатами:
В Consumer, если у каждой записи есть внутренний ID, мы можем использовать его для атомарного сохранения результата в БД. Однако это не всегда возможно. Например, если Consumer не работает с БД со сторонним АПИ, у которого нет идемпотентности.  
При помощи настройки enable.idempotence в true: каждый Producer получает свой уникальный ID на брокере, а также инициализируется SEQ number. Producer отправляет сообщения с возрастающим SEQ, а брокер сверяет что каждое полученное сообщение обладает правильным SEQ (0,1,2,3...). Таким образом если отправляется дубликат — брокер отклоняет его, поскольку видит неправильный SEQ. Важно помнить, что используя idempotence, вы не можете выставить max.in.flight.requests.per.connection больше чем 5, а retiries в 0. Кроме того, опция acks должна быть равна all.
 

#### Consumer:
Библиотека Consumer в Kafka используется для чтения данных из топиков. Клиент Consumer автоматически обрабатывает падения брокеров и перемещения лидеров партиций в кластере.  
- enable.auto.commit — автоматические коммиты с интервалом auto.commit.interval.ms.  
- session.timeout.ms — максимальное время между heartbeat запросами к брокеру (контролируется брокером).  
- max.poll.interval.ms — максимальное время между вызовами poll (контролируется самим клиентом).  
- group.id — идентификатор группы.  
- group.instance.id — статический идентификатор консьюмера  
Рестарт должен уложиться в session.timeout.ms  
 
#### Transactions & Exactly Once:
Созданы для решения проблем обработки в “read-process-write” приложениях, где “read & write” производятся в/из Kafka.  
Традиционно выбор был из двух режимов обработки  
- at-least once — вначале мы “процессим” сообщение и затем “коммитим” оффсет в Кафку (возможна повторная обработка);
- at-most once — вначале “коммитим” оффсет и только затем “процессим” сообщение (возможна потеря данных).
- Все хотят режим exactly once — возможность обработки сообщений только 1 раз. Именно тут на помощь приходят Transactions.
 
#### Главные принципы транзакционности в Кафке:
- Атомарная запись в несколько партиций одновременно.
- Защита от “зомби” из коробки.
- Изоляция: Consumers получают сообщения только успешно завершенных транзакций.
 
“Подводные камни”, связанные с Transactions:
- Транзакционные маркеры — обычные сообщения, которые могут несколько ломать логику поиска сообщений по timestamp (offsetsByTime()), если вы пользуетесь CreateTime.
- Ущерб производительности на мелких стримах не заметен (±3% degradation, 1KB of records per/sec). Растет нагрузка на брокеры с ростом количества транзакций. Совет: использовать Transactions только там где надо (не включать по-умолчанию).
Открытые транзакции не дают Consumers с isolation.level=read_committed читать сообщения выше Last Stable Offset (LSO).  
Transactions обеспечивают exactly-once только в “read-process-write” приложениях читающих и пишущих в Кафку.  

### Как сделать кластер отказоустойчивым:
- Отказоустойчивый кластер содержит минимум 3 зукипера;
- Отказоустойчивый кластер содержит минимум 2 брокера;
- Кластер следует размещать на разных серверах;
- Сервера должны размещаться также на разных стойках.
replication.factor # копии на другие брокеры  
brocker.rack # разные реплики на разные стойки  
acks=1(default),0,-1 # Ждем ответа от лидера партиции, не ждем, ждем ответ от min.insync.replicas  
*Хороший default:* min.insync.replicas=2, replication.factor=2  
replica.lag.time.max.ms # Возможное время отставания реплики  
#### Controller:
Kafka является распределенной системой, и контроллер решает задачу по координации этой системы в случае различных изменений в ее состоянии. Контроллером может стать абсолютно любой брокер в кластере.  

Контроллер отвечает за:
- За создание и удаление топиков;
- За добавление партиций и назначение им лидеров;
- За разрешение ситуаций с падениями брокеров или выходом их из кластера.

В обязанности контроллера также входит:  
- Создание новых партиций и топиков;
- Удаление топиков. 

#### Конфигурации брокеров, помогающие сделать кластер более отказоустойчивым:  
Данная группа настроек помогает держать нагрузку на брокеры кластера равномерной (даже после рестартов и падений). Эти настройки включены по умолчанию:  
- auto.leader.rebalance.enable # Перевыбор лидеров для равномерной нагрузки
- leader.imbalance.check.interval.seconds
- leader.imbalance.per.broker.percentage

Параметры частоты записи данных на жесткий диск:  
- flush.messages
- flush.ms

Ограничения по числу подключений:  
- max.connections
- max.connections.per.ip
- max.connection.creation.rate

Количество партиций лога по умолчанию (для каждого топика).  
- Num.partitions
Настройка числа партиций - не больше 4к партиций на брокер и не больше 200к партиций на кластер. Если партиций становится слишком много, то после жесткого падения ваш кластер может восстанавливаться продолжительное время (например, десятки минут). Это делает администрирование кластера более трудоемкой задачей.  

#### Бэкапы:
**Бэкапы Zookeeper являются обязательными!**  
Важно валидировать бэкапы в Zookeeper для проверки их работы (это легко и просто, поскольку бэкапы в Zookeeper легковесные).  
В случае Кафки в большинстве кейсов достаточно иметь репликацию данных (а данные уже не являются легковесными).  
Помните, что все кейсы разные, и вам нужно самостоятельно продумывать является ли оправданным или нет использование бэкапов в Kafka.  

#### Практики, используемые для еще большего повышения отказоустойчивости:  
- Disaster recovery plan # Инструкция на случай аварии
- Disaster тесты # Практика, ломаем и чиним контроллируемо
- Runbooks и on-call дежурства
- Knowledge sharing и bus-factor 2+
- Разбор инцидентов

### Преимущества Stretched кластера (один растянутый географически кластер):  
- Синхронная репликация;
- Концептуально простой;
- Нет проблем с failover;
- Есть поддержка локального чтения (replica.selector.class=RackAwareReplicaSelector, client.rack и brocker.rack).
 
Минусы, при использовании Stretched кластера:  
- Работает только при низком latency (<=30ms);
- Нет поддержки локальной записи;
- Полный отказ кластера = даунтайм.
 
### Особенности асинхронного кластера (содержащего репликатор):  
- Является логическим объединением нескольких физических кластеров, синхронизируемых между собой отдельной технологией, которая называется репликатор;
- Существует риск потери части данных, но работает даже при больших задержках
- Невозможно совершить failover на другой кластер, если наш основной кластер упал;
- Имеет достаточно сложную архитектуру.
 
Существующие основные репликаторы:  
- MirrorMaker
- MirrorMaker 2.0 (то, что надо!)
- Uber Replicator
- Confluent Replicator
