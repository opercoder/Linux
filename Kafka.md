### Внутренняя структура данных Kafka состоит из:
1. **Event (сообщение)**, который включает в себя: ключ (key), значение (value), timestamp и опциональный набор метаданных (headers);  
1. **Topics (топики)**, в которых организованы и хранятся сообщения. В свою очередь, каждый Topic состоит из одной или более партиций;
1. **Partitions (партиции)** - это распределенный отказоустойчивый лог (Log). Сообщения с одинаковыми ключами записываются в одну и ту же партицию;
1. У каждой партиции есть один брокер лидер - **Leader** (принимает сообщения от Producer и в общем случае отдает сообщения консьюмеру); 
1. **Фолловеры (Follower)** являются брокерами, которые хранят реплику всех данных партиции и осуществляют запросы лидеру.

#### Клиентские библиотеки:
Основные способы взаимодействия с кластером.  
Официальным языком и клиентом Kafka является Java. Для других языков также создано множество библиотек.  
Все брокеры кластера могут быть помещены за один IP/Load Balancer для Service Discovery. 
При этом нужно обеспечить прямой доступ клиентов к каждому из них.  
Подробное описание протокола Apache Kafka доступно в официальной документации: https://kafka.apache.org/protocol  

#### Producer:
Посылает сообщения брокеру батчами, улучшая пропускную способность и степень сжатия данных.  
linger.ms — аналог Nagle’s Algorithm из TCP.  
Внутренний буфер ограниченного размера — buffer.memory. При заполнении блокирует отправку на max.block.ms.  
delivery.timeout.ms — общий таймаут на доставку, равный 2-м минутам по умолчанию. В пределах этого лимита клиент будет пробовать доставить сообщение заново в случае ошибок.  
При помощи опции Retries можно контролировать количество попыток доставки. Важно помнить, что Retries могут сломать очередность сообщений если max.in.flight.requests.per.connection > 1.  
 
#### Как бороться с дубликатами:
В Consumer, если у каждой записи есть внутренний ID, мы можем использовать его для атомарного сохранения результата в БД. Однако это не всегда возможно. Например, если Consumer не работает с БД со сторонним АПИ, у которого нет идемпотентности.  
При помощи настройки enable.idempotence в true: каждый Producer получает свой уникальный ID на брокере, а также инициализируется SEQ number. Producer отправляет сообщения с возрастающим SEQ, а брокер сверяет что каждое полученное сообщение обладает правильным SEQ (0,1,2,3...). Таким образом если отправляется дубликат — брокер отклоняет его, поскольку видит неправильный SEQ. Важно помнить, что используя idempotence, вы не можете выставить max.in.flight.requests.per.connection больше чем 5, а retiries в 0. Кроме того, опция acks должна быть равна all.
 

#### Consumer:
Библиотека Consumer в Kafka используется для чтения данных из топиков. Клиент Consumer автоматически обрабатывает падения брокеров и перемещения лидеров партиций в кластере.  
- enable.auto.commit — автоматические коммиты с интервалом auto.commit.interval.ms.  
- session.timeout.ms — максимальное время между heartbeat запросами к брокеру (контролируется брокером).  
- max.poll.interval.ms — максимальное время между вызовами poll (контролируется самим клиентом).  
- group.id — идентификатор группы.  
- group.instance.id — статический идентификатор консьюмера  
Рестарт должен уложиться в session.timeout.ms  
 
#### Transactions & Exactly Once:
Созданы для решения проблем обработки в “read-process-write” приложениях, где “read & write” производятся в/из Kafka.  
Традиционно выбор был из двух режимов обработки  
- at-least once — вначале мы “процессим” сообщение и затем “коммитим” оффсет в Кафку (возможна повторная обработка);
- at-most once — вначале “коммитим” оффсет и только затем “процессим” сообщение (возможна потеря данных).
- Все хотят режим exactly once — возможность обработки сообщений только 1 раз. Именно тут на помощь приходят Transactions.
 
#### Главные принципы транзакционности в Кафке:
- Атомарная запись в несколько партиций одновременно.
- Защита от “зомби” из коробки.
- Изоляция: Consumers получают сообщения только успешно завершенных транзакций.
 
“Подводные камни”, связанные с Transactions:
- Транзакционные маркеры — обычные сообщения, которые могут несколько ломать логику поиска сообщений по timestamp (offsetsByTime()), если вы пользуетесь CreateTime.
- Ущерб производительности на мелких стримах не заметен (±3% degradation, 1KB of records per/sec). Растет нагрузка на брокеры с ростом количества транзакций. Совет: использовать Transactions только там где надо (не включать по-умолчанию).
Открытые транзакции не дают Consumers с isolation.level=read_committed читать сообщения выше Last Stable Offset (LSO).  
Transactions обеспечивают exactly-once только в “read-process-write” приложениях читающих и пишущих в Кафку.  

### Как сделать кластер отказоустойчивым:
- Отказоустойчивый кластер содержит минимум 3 зукипера;
- Отказоустойчивый кластер содержит минимум 2 брокера;
- Кластер следует размещать на разных серверах;
- Сервера должны размещаться также на разных стойках.
replication.factor # копии на другие брокеры  
brocker.rack # разные реплики на разные стойки  
acks=1(default),0,-1 # Ждем ответа от лидера партиции, не ждем, ждем ответ от min.insync.replicas  
*Хороший default:* min.insync.replicas=2, replication.factor=2  
replica.lag.time.max.ms # Возможное время отставания реплики  
#### Controller:
Kafka является распределенной системой, и контроллер решает задачу по координации этой системы в случае различных изменений в ее состоянии. Контроллером может стать абсолютно любой брокер в кластере.  

Контроллер отвечает за:
- За создание и удаление топиков;
- За добавление партиций и назначение им лидеров;
- За разрешение ситуаций с падениями брокеров или выходом их из кластера.

В обязанности контроллера также входит:  
- Создание новых партиций и топиков;
- Удаление топиков. 

#### Конфигурации брокеров, помогающие сделать кластер более отказоустойчивым:  
Данная группа настроек помогает держать нагрузку на брокеры кластера равномерной (даже после рестартов и падений). Эти настройки включены по умолчанию:  
- auto.leader.rebalance.enable # Перевыбор лидеров для равномерной нагрузки
- leader.imbalance.check.interval.seconds
- leader.imbalance.per.broker.percentage

Параметры частоты записи данных на жесткий диск:  
- flush.messages
- flush.ms

Ограничения по числу подключений:  
- max.connections
- max.connections.per.ip
- max.connection.creation.rate

Количество партиций лога по умолчанию (для каждого топика).  
- Num.partitions
Настройка числа партиций - не больше 4к партиций на брокер и не больше 200к партиций на кластер. Если партиций становится слишком много, то после жесткого падения ваш кластер может восстанавливаться продолжительное время (например, десятки минут). Это делает администрирование кластера более трудоемкой задачей.  

## Бэкапы:
**Бэкапы Zookeeper являются обязательными!**  
Важно валидировать бэкапы в Zookeeper для проверки их работы (это легко и просто, поскольку бэкапы в Zookeeper легковесные).  
В случае Кафки в большинстве кейсов достаточно иметь репликацию данных (а данные уже не являются легковесными).  
Помните, что все кейсы разные, и вам нужно самостоятельно продумывать является ли оправданным или нет использование бэкапов в Kafka.  

#### Практики, используемые для еще большего повышения отказоустойчивости:  
- Disaster recovery plan # Инструкция на случай аварии
- Disaster тесты # Практика, ломаем и чиним контроллируемо
- Runbooks и on-call дежурства
- Knowledge sharing и bus-factor 2+
- Разбор инцидентов

### Преимущества Stretched кластера (один растянутый географически кластер):  
- Синхронная репликация;
- Концептуально простой;
- Нет проблем с failover;
- Есть поддержка локального чтения (replica.selector.class=RackAwareReplicaSelector, client.rack и brocker.rack).
 
Минусы, при использовании Stretched кластера:  
- Работает только при низком latency (<=30ms);
- Нет поддержки локальной записи;
- Полный отказ кластера = даунтайм.
 
### Особенности асинхронного кластера (содержащего репликатор):  
- Является логическим объединением нескольких физических кластеров, синхронизируемых между собой отдельной технологией, которая называется репликатор;
- Существует риск потери части данных, но работает даже при больших задержках
- Невозможно совершить failover на другой кластер, если наш основной кластер упал;
- Имеет достаточно сложную архитектуру.
 
### Существующие основные репликаторы:  
- MirrorMaker
- MirrorMaker 2.0 (то, что надо!)
- Uber Replicator
- Confluent Replicator

## Мониторинг Kafka:  
- Отслеживание поведения системы в реальном времени;
- Трекинг изменений в поведении с течением времени;
- Заблаговременное планирование ресурсов.
 
#### Что необходимо мониторить:  
- Собирать метрики Apache Kafka и Apache Zookeeper;
- Собирать метрики JVM;
- Собирать метрики хоста системы (загрузки памяти, ЦПУ сетевого стека, диска).
 
#### Мониторинг Kafka - основные тезисы:
- Кафка репортит несколько сотен метрик из коробки;
- По умолчанию, метрики репортятся через JMX;
- Можно подключать кастомные репортеры, изменяя настройку metric.reporters.
 
#### Варианты архитектуры (несколько способов сбора и отображения метрик из Kafka):  
- JmxTrans + Graphite/StatsD/TSDB: https://github.com/jmxtrans/jmxtrans
- Prometheus & JMX Exporter: https://github.com/prometheus/jmx_exporter
- Custom Reporter
 
#### Ключевые метрики Kafka:  
ЗАПОЛНИТЬ   

#### Ключевые метрики Zookeeper:  
ЗАПОЛНИТЬ   


#### Мониторинг клиентов и сбор метрик с клиентских библиотек:   
- Клиенты Кафки также репортят кучу метрик из коробки;
- Желательно оборачивать “ванильных клиентов” в собственную библиотеку;
- Для мониторинга лага консюмеров лучше использовать https://github.com/linkedin/Burrow
 
#### SLI:  
SLI (Service level indicator) - ключевая метрика работоспособности сервиса;  
Доступность записи = (Total Produce Requests - Produce Request Errors) / Total Produce Requests * 100%  
#### SLO:  
SLO (Service Level Objective) - целевое значение SLI;  
Повышает стабильность конечного продукта и помогает в приоритизации нашей работы;  
Выступает в следующих гарантиях, что доступность записи >= 99.9% в течение последних 14 дней.  
 
#### Как считать SLI/SLO:  
- Использование метрик внешнего монитора Xinfra Monitor: https://github.com/linkedin/kafka-monitor

## Анализ производительности

#### “Быстрая Kafka”:
- Kafka оптимизирована под высокую пропускную способность, т.е. заточена под то, чтобы передавать, как можно больше данных за единицу времени.  
- При этом, у нее сравнительно высокий и неравномерный latency;
- Подходит идеально для синхронных систем, но в системах, где требуется гарантия latency лучше не использовать Kafka.
 
#### Что делает Kafka быстрой и повышает ее производительность:
- Линейный I/O (read-ahead, write-behind);
- Batching & Compression;
- Дешевые консьюмеры;
- Нет fsync;
- Zero Copy & Pagecache.
 
#### Append-Only Log:
- Основная структура данных или Append Only Log является главной фундаментальной особенностью Kafka, позволяющая ей добиваться столь высокой пропускной способностью;
- append-only подразумевает линейный (sequential) I/O;
- read-ahead — данные при чтении предзагружаются системой;
- write-behind — мелкие логические операции записи группируются в большие физические;
- 6 дешевых 7200 RPM SATA дисков в RAID-5 способны линейно записать 600MB в секунду
- Линейный доступ к диску может быть быстрее чтения из памяти: https://queue.acm.org/detail.cfm?id=1563874
 
#### Batching & Compression
- Являются еще одними важнейшими оптимизациями Kafka;
- Кафка аккумулирует записи в батчи перед отправкой, снижая количество запросов к диску и пакетов, летящих по сети;
- Встроенная компрессия батчей дополнительно снижает нагрузку на сеть и диск и улучшает пропускную способность.
 
#### Легковесные консьюмеры:
- Также позволяют Kafka добиваться дополнительной пропускной способности и высокой производительности;
- Консьюмеры Кафки при чтении не меняют данные;
- Коммит консьюмера — запись в конец топика \__consumer_offsets;
- Real-time консьюмеры фактически читают данные из pagecache брокера.
 
#### Инструменты бенчмаркинга:
- Встроенная утилита: bin/kafka-producer-perf-test.sh
- Встроенная утилита: bin/kafka-consumer-perf-test.sh
- Встроенная утилита: bin/kafka-run-class.sh kafka.tools.EndToEndLatency
- Trogdor: github.com/apache/kafka/blob/trunk/TROGDOR.md
- OpenMessaging Benchmark: openmessaging.cloud/docs/benchmarks/kafka/

## Поддержка работоспособности кластера и траблшутинг
#### Автоматическая балансировка нагрузки:
- Автоматически производит балансировку, не требуя от вас дополнительной работы;
- Происходит в любое время (это одновременно и плюс, и риск автоматической балансировки);
- Редко используется на практике (особенно для больших кластеров): часто хорошей идеей будет отключать эту настройку и выполнять выбор предпочитаемых лидеров в управляемом режиме.
 
#### Ручная балансировка нагрузки:
- Происходит контролируемо - можно настроить время ребалансировки;
- Дает возможность принять более правильное решение по поводу необходимости балансировки в данный момент;
- Возможность “умной” автоматизации;
- Требует алертинг.
 
#### Балансировка партиций по брокерам:
- Данный механизм  производит реальное движение данных между брокерами, т.е. перенос всей информации, которая была записана в партицию;
- Лучше не использовать без троттлинга;
- Ребалансировка требуется, как при добавлении, так и при удалении брокера.
 
#### Зачем обновлять версии кластера:
- Для устранения известных багов и ускорения работы кластера (повышения его производительности);
- Появление нового функционала (новые фичи);
- SLO по обновлению;
- Другие причины, например престиж команды, и так далее.

#### Последовательность обновления кластера Kafka:
- Составление подробного план обновления;
- Фиксация версии протокола брокеров;
- Проведение rolling restart брокеров;
- Обновление версии протокола брокеров (этот шаг является точкой невозврата!);
- Обновление версии формата клиентов;
- Zookeeper - не забывайте проверять его обновления!
Документация по обновлению: https://kafka.apache.org/documentation/#upgrade
